
#Cleaning and preprocessing text
#Two ways to make a corpus, you can focus on cleaning, or preprocessing, the text. First, you'll clean a small piece of text, then you will move on to larger corpora.

> # Create the object: text
> text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."
> 
> # Make lowercase
> tolower(text)
[1] "<b>she</b> woke up at       6 a.m. it's so early!  she was only 10% awake and began drinking coffee in front of her computer."
> 

# Word stemming and stem completion on a sentence
> # Remove punctuation
> removePunctuation(text)
[1] "bSheb woke up at       6 AM Its so early  She was only 10 awake and began drinking coffee in front of her computer"
> 
> # Remove numbers
> removeNumbers(text)
[1] "<b>She</b> woke up at        A.M. It's so early!  She was only % awake and began drinking coffee in front of her computer."
> # Remove whitespace
> stripWhitespace(text)
[1] "<b>She</b> woke up at 6 A.M. It's so early! She was only 10% awake and began drinking coffee in front of her computer."
# Cleaning with qdap
> head(text)
[1] "<b>She</b> woke up at       6 A.M. It's so early!  She was only 10% awake and began drinking coffee in front of her computer."
> # Remove text within brackets
> bracketX(text)
[1] "She woke up at 6 A.M. It's so early! She was only 10% awake and began drinking coffee in front of her computer."
> 
> # Replace numbers with words
> replace_number(text)
[1] "<b>She</b> woke up at six A.M. It's so early! She was only ten% awake and began drinking coffee in front of her computer."
> # Replace abbreviations
> replace_abbreviation(text)
[1] "<b>She</b> woke up at 6 AM It's so early! She was only 10% awake and began drinking coffee in front of her computer."
> 
> # Replace contractions
> replace_contraction(text)
[1] "<b>She</b> woke up at 6 A.M. it is so early! She was only 10% awake and began drinking coffee in front of her computer."
> 
> # Replace symbols with words
> replace_symbol(text)
[1] "<b>She</b> woke up at 6 A.M. It's so early! She was only 10 percent awake and began drinking coffee in front of her computer."
> ## stop wordsÑŽ Often there are words that are frequent but provide little information
> text
[1] "<b>She</b> woke up at       6 A.M. It's so early!  She was only 10% awake and began drinking coffee in front of her computer."
> # List standard English stop words
> stopwords("en")
  [1] "i"          "me"         "my"         "myself"     "we"        
  [6] "our"        "ours"       "ourselves"  "you"        "your"      
 [11] "yours"      "yourself"   "yourselves" "he"         "him"       
 [16] "his"        "himself"    "she"        "her"        "hers"      
 [21] "herself"    "it"         "its"        "itself"     "they"      
 [26] "them"       "their"      "theirs"     "themselves" "what"      
 [31] "which"      "who"        "whom"       "this"       "that"      
 [36] "these"      "those"      "am"         "is"         "are"       
 [41] "was"        "were"       "be"         "been"       "being"     
 [46] "have"       "has"        "had"        "having"     "do"        
 [51] "does"       "did"        "doing"      "would"      "should"    
 [56] "could"      "ought"      "i'm"        "you're"     "he's"      
 [61] "she's"      "it's"       "we're"      "they're"    "i've"      
 [66] "you've"     "we've"      "they've"    "i'd"        "you'd"     
 [71] "he'd"       "she'd"      "we'd"       "they'd"     "i'll"      
 [76] "you'll"     "he'll"      "she'll"     "we'll"      "they'll"   
 [81] "isn't"      "aren't"     "wasn't"     "weren't"    "hasn't"    
 [86] "haven't"    "hadn't"     "doesn't"    "don't"      "didn't"    
 [91] "won't"      "wouldn't"   "shan't"     "shouldn't"  "can't"     
 [96] "cannot"     "couldn't"   "mustn't"    "let's"      "that's"    
[101] "who's"      "what's"     "here's"     "there's"    "when's"    
[106] "where's"    "why's"      "how's"      "a"          "an"        
[111] "the"        "and"        "but"        "if"         "or"        
[116] "because"    "as"         "until"      "while"      "of"        
[121] "at"         "by"         "for"        "with"       "about"     
[126] "against"    "between"    "into"       "through"    "during"    
[131] "before"     "after"      "above"      "below"      "to"        
[136] "from"       "up"         "down"       "in"         "out"       
[141] "on"         "off"        "over"       "under"      "again"     
[146] "further"    "then"       "once"       "here"       "there"     
[151] "when"       "where"      "why"        "how"        "all"       
[156] "any"        "both"       "each"       "few"        "more"      
[161] "most"       "other"      "some"       "such"       "no"        
[166] "nor"        "not"        "only"       "own"        "same"      
[171] "so"         "than"       "too"        "very"
> 
> # Print text without standard stop words
> removeWords(text, stopwords("en"))
[1] "<b>She</b> woke         6 A.M. It's  early!  She   10% awake  began drinking coffee  front   computer."
> 
> # Add "coffee" and "bean" to the list: new_stops
> new_stops <- c("coffee", "bean", stopwords("en"))
> 
> # Remove stop words from text
> removeWords(text, new_stops)
[1] "<b>She</b> woke         6 A.M. It's  early!  She   10% awake  began drinking   front   computer."
> 
> # Create complicate
> complicate <- c( "complicated", "complication", "complicatedly")
> 
> # Perform word stemming: stem_doc
> stem_doc <- stemDocument(complicate)
> stem_doc
[1] "complic" "complic" "complic"
> # Create the completion dictionary: comp_dict
> comp_dict <- "complicate"
> 
> # Perform stem completion: complete_text
> complete_text <- stemCompletion(stem_doc, comp_dict)
> 
> # Print complete_text
> complete_text
     complic      complic      complic 
"complicate" "complicate" "complicate"
> 
> text_data
[1] "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."
> rm_punc <- removePunctuation(text_data)
> rm_punc
[1] "In a complicated haste Tom rushed to fix a new complication too complicatedly"
> # Create character vector: n_char_vec
> n_char_vec <- unlist(strsplit(rm_punc, split = " "))
> n_char_vec
 [1] "In"            "a"             "complicated"   "haste"        
 [5] "Tom"           "rushed"        "to"            "fix"          
 [9] "a"             "new"           "complication"  "too"          
[13] "complicatedly"
> # Perform word stemming: stem_doc
> stem_doc <- stemDocument(n_char_vec)
> 
> # Print stem_doc
> 
> complete_doc <- stemCompletion(stem_doc, comp_dict)
> 
> # Print complete_doc
> complete_doc
          In            a      complic         hast          Tom         rush 
        "In"          "a" "complicate"      "haste"        "Tom"       "rush" 
          to          fix            a          new      complic          too 
        "to"        "fix"          "a"        "new" "complicate"        "too" 
     complic 
"complicate"
# Make a function including all cleaning data
clean_corpus <- function(corpus) {
  # Remove punctuation
  corpus <- tm_map(corpus, removePunctuation)
  # Transform to lower case
  corpus <- tm_map(corpus, content_transformer(tolower))
  # Add more stopwords
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee", "mug"))
  # Strip whitespace
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}
> # Apply this function to the text
> clean_corpus <- function(corpus){
    corpus <- tm_map(corpus, removePunctuation)
    corpus <- tm_map(corpus, content_transformer(tolower))
    corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee", "mug"))
    corpus <- tm_map(corpus, stripWhitespace)
    return(corpus)
  }
> 
> # Appling customized function to the tweet_corp: clean_corp
> clean_corp <- clean_corpus(tweet_corp)
> 
> # Print out a cleaned up tweet
> clean_corp[[227]][1]
$content
[1] "also dogs arent smart enough dip donut eat part thats dipped ladyandthetramp"
> 
> # Print out the same tweet in original form
> tweets$text[227]
[1] "Also, dogs aren't smart enough to dip the donut in the coffee and then eat the part that's been dipped. #ladyandthetramp"
