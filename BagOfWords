
#Cleaning and preprocessing text
#Two ways to make a corpus, you can focus on cleaning, or preprocessing, the text. First, you'll clean a small piece of text, then you will move on to larger corpora.

> # Create the object: text
> text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."
> 
> # Make lowercase
> tolower(text)
[1] "<b>she</b> woke up at       6 a.m. it's so early!  she was only 10% awake and began drinking coffee in front of her computer."
> 

# Word stemming and stem completion on a sentence
> # Remove punctuation
> removePunctuation(text)
[1] "bSheb woke up at       6 AM Its so early  She was only 10 awake and began drinking coffee in front of her computer"
> 
> # Remove numbers
> removeNumbers(text)
[1] "<b>She</b> woke up at        A.M. It's so early!  She was only % awake and began drinking coffee in front of her computer."
> # Remove whitespace
> stripWhitespace(text)
[1] "<b>She</b> woke up at 6 A.M. It's so early! She was only 10% awake and began drinking coffee in front of her computer."
# Cleaning with qdap
> head(text)
[1] "<b>She</b> woke up at       6 A.M. It's so early!  She was only 10% awake and began drinking coffee in front of her computer."
> # Remove text within brackets
> bracketX(text)
[1] "She woke up at 6 A.M. It's so early! She was only 10% awake and began drinking coffee in front of her computer."
> 
> # Replace numbers with words
> replace_number(text)
[1] "<b>She</b> woke up at six A.M. It's so early! She was only ten% awake and began drinking coffee in front of her computer."
> # Replace abbreviations
> replace_abbreviation(text)
[1] "<b>She</b> woke up at 6 AM It's so early! She was only 10% awake and began drinking coffee in front of her computer."
> 
> # Replace contractions
> replace_contraction(text)
[1] "<b>She</b> woke up at 6 A.M. it is so early! She was only 10% awake and began drinking coffee in front of her computer."
> 
> # Replace symbols with words
> replace_symbol(text)
[1] "<b>She</b> woke up at 6 A.M. It's so early! She was only 10 percent awake and began drinking coffee in front of her computer."
> ## stop wordsю Often there are words that are frequent but provide little information
> text
[1] "<b>She</b> woke up at       6 A.M. It's so early!  She was only 10% awake and began drinking coffee in front of her computer."
> # List standard English stop words
> stopwords("en")
  [1] "i"          "me"         "my"         "myself"     "we"        
  [6] "our"        "ours"       "ourselves"  "you"        "your"      
 [11] "yours"      "yourself"   "yourselves" "he"         "him"       
 [16] "his"        "himself"    "she"        "her"        "hers"      
 [21] "herself"    "it"         "its"        "itself"     "they"      
 [26] "them"       "their"      "theirs"     "themselves" "what"      
 [31] "which"      "who"        "whom"       "this"       "that"      
 [36] "these"      "those"      "am"         "is"         "are"       
 [41] "was"        "were"       "be"         "been"       "being"     
 [46] "have"       "has"        "had"        "having"     "do"        
 [51] "does"       "did"        "doing"      "would"      "should"    
 [56] "could"      "ought"      "i'm"        "you're"     "he's"      
 [61] "she's"      "it's"       "we're"      "they're"    "i've"      
 [66] "you've"     "we've"      "they've"    "i'd"        "you'd"     
 [71] "he'd"       "she'd"      "we'd"       "they'd"     "i'll"      
 [76] "you'll"     "he'll"      "she'll"     "we'll"      "they'll"   
 [81] "isn't"      "aren't"     "wasn't"     "weren't"    "hasn't"    
 [86] "haven't"    "hadn't"     "doesn't"    "don't"      "didn't"    
 [91] "won't"      "wouldn't"   "shan't"     "shouldn't"  "can't"     
 [96] "cannot"     "couldn't"   "mustn't"    "let's"      "that's"    
[101] "who's"      "what's"     "here's"     "there's"    "when's"    
[106] "where's"    "why's"      "how's"      "a"          "an"        
[111] "the"        "and"        "but"        "if"         "or"        
[116] "because"    "as"         "until"      "while"      "of"        
[121] "at"         "by"         "for"        "with"       "about"     
[126] "against"    "between"    "into"       "through"    "during"    
[131] "before"     "after"      "above"      "below"      "to"        
[136] "from"       "up"         "down"       "in"         "out"       
[141] "on"         "off"        "over"       "under"      "again"     
[146] "further"    "then"       "once"       "here"       "there"     
[151] "when"       "where"      "why"        "how"        "all"       
[156] "any"        "both"       "each"       "few"        "more"      
[161] "most"       "other"      "some"       "such"       "no"        
[166] "nor"        "not"        "only"       "own"        "same"      
[171] "so"         "than"       "too"        "very"
> 
> # Print text without standard stop words
> removeWords(text, stopwords("en"))
[1] "<b>She</b> woke         6 A.M. It's  early!  She   10% awake  began drinking coffee  front   computer."
> 
> # Add "coffee" and "bean" to the list: new_stops
> new_stops <- c("coffee", "bean", stopwords("en"))
> 
> # Remove stop words from text
> removeWords(text, new_stops)
[1] "<b>She</b> woke         6 A.M. It's  early!  She   10% awake  began drinking   front   computer."
> 
> # Create complicate
> complicate <- c( "complicated", "complication", "complicatedly")
> 
> # Perform word stemming: stem_doc
> stem_doc <- stemDocument(complicate)
> stem_doc
[1] "complic" "complic" "complic"
> # Create the completion dictionary: comp_dict
> comp_dict <- "complicate"
> 
> # Perform stem completion: complete_text
> complete_text <- stemCompletion(stem_doc, comp_dict)
> 
> # Print complete_text
> complete_text
     complic      complic      complic 
"complicate" "complicate" "complicate"
> 
> text_data
[1] "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."
> rm_punc <- removePunctuation(text_data)
> rm_punc
[1] "In a complicated haste Tom rushed to fix a new complication too complicatedly"
> # Create character vector: n_char_vec
> n_char_vec <- unlist(strsplit(rm_punc, split = " "))
> n_char_vec
 [1] "In"            "a"             "complicated"   "haste"        
 [5] "Tom"           "rushed"        "to"            "fix"          
 [9] "a"             "new"           "complication"  "too"          
[13] "complicatedly"
> # Perform word stemming: stem_doc
> stem_doc <- stemDocument(n_char_vec)
> 
> # Print stem_doc
> 
> complete_doc <- stemCompletion(stem_doc, comp_dict)
> 
> # Print complete_doc
> complete_doc
          In            a      complic         hast          Tom         rush 
        "In"          "a" "complicate"      "haste"        "Tom"       "rush" 
          to          fix            a          new      complic          too 
        "to"        "fix"          "a"        "new" "complicate"        "too" 
     complic 
"complicate"
# Make a function including all cleaning data
clean_corpus <- function(corpus) {
  # Remove punctuation
  corpus <- tm_map(corpus, removePunctuation)
  # Transform to lower case
  corpus <- tm_map(corpus, content_transformer(tolower))
  # Add more stopwords
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee", "mug"))
  # Strip whitespace
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}
> # Apply this function to the text
> clean_corpus <- function(corpus){
    corpus <- tm_map(corpus, removePunctuation)
    corpus <- tm_map(corpus, content_transformer(tolower))
    corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee", "mug"))
    corpus <- tm_map(corpus, stripWhitespace)
    return(corpus)
  }
> 
> # Appling customized function to the tweet_corp: clean_corp
> clean_corp <- clean_corpus(tweet_corp)
> 
> # Print out a cleaned up tweet
> clean_corp[[227]][1]
$content
[1] "also dogs arent smart enough dip donut eat part thats dipped ladyandthetramp"
> 
> # Print out the same tweet in original form
> tweets$text[227]
[1] "Also, dogs aren't smart enough to dip the donut in the coffee and then eat the part that's been dipped. #ladyandthetramp"
> # Create the dtm from the corpus: coffee_dtm
> 
> coffee_dtm <- DocumentTermMatrix(clean_corp)
> # Print out coffee_dtm data
> coffee_dtm
<<DocumentTermMatrix (documents: 1000, terms: 3101)>>
Non-/sparse entries: 7724/3093276
Sparsity           : 100%
Maximal term length: 33
Weighting          : term frequency (tf)
> 
> 
> # Convert coffee_dtm to a matrix: coffee_m
> coffee_m <- as.matrix(coffee_dtm)
> 
> # Print the dimensions of coffee_m
> dim(coffee_m)
[1] 1000 3101
> 
> # Review a portion of the matrix to get some Starbucks
> print(coffee_m[475:478, 2593:2594])
     Terms
Docs  star starbucks
  475    0         0
  476    0         0
  477    0         0
  478    0         2
> 
> # Create a TDM from clean_corp: coffee_tdm
> 
> coffee_tdm <- TermDocumentMatrix(clean_corp)
> # Print coffee_tdm data
> coffee_tdm
<<TermDocumentMatrix (terms: 3101, documents: 1000)>>
Non-/sparse entries: 7724/3093276
Sparsity           : 100%
Maximal term length: 33
Weighting          : term frequency (tf)
> 
> 
> # Convert coffee_tdm to a matrix: coffee_m
> coffee_m <- as.matrix(coffee_tdm)
> 
> # Print the dimensions of the matrix
> dim(coffee_m)
[1] 3101 1000
> 
> # Review a portion of the matrix
> coffee_m[2593:2594, 475:478]
           Docs
Terms       475 476 477 478
  star        0   0   0   0
  starbucks   0   0   0   2
> 
> ## coffee_tdm is still loaded in your workspace
> 
> # Create a matrix: coffee_m
> coffee_m  <-as.matrix(coffee_tdm)
> 
> # Calculate the rowSums: term_frequency
> term_frequency <- rowSums(coffee_m)
> 
> # Sort term_frequency in descending order
> term_frequency<- sort(term_frequency, decreasing = TRUE)
> 
> # View the top 10 most common words
> head(term_frequency,10)
    like      cup      one     shop     just      get  morning     want 
     111      103       79       69       66       62       57       49 
drinking      can 
      47       45
> 
> # term_frequency[1:10]
> # Plot a barchart of the 10 most common words
> barplot(term_frequency[1:10], col = "tan", las = 2)
> 
> # Make a frequency barchart
> plot(frequency)
> # fast way to get frequent terms is with freq_terms() from qdap
> frequency <- freq_terms(
    tweets$text, 
    top = 10, 
    at.least = 3, 
    stopwords = "Top200Words"
  )
> 
> # Make a frequency barchart
> plot(frequency)

# Make a frequency barchart
plot(frequency)
# Make a frequency barchart
> # Create frequency
> frequency <- freq_terms(
    tweets$text, 
    top = 10, 
    at.least = 3, 
    stopwords = stopwords("english")
  )
> 
> # Make a frequency barchart
> plot(frequency)
library(wordcloud)
> # Load wordcloud package
> 
> 
> # Print the first 10 entries in term_frequency
> term_frequency[1:10]
chardonnay     marvin       wine       gaye       just      glass       like 
       824        104         83         76         75         63         52 
    bottle        lol     little 
        47         43         35
> 
> # Vector of terms
> terms_vec<-names(term_frequency)
> > # Review a "cleaned" tweet
#head(content(chardonnay_corp))
> # Create a wordcloud for the values in word_freqs
> wordcloud(terms_vec, as.matrix(term_frequency), max.words = 50, colors = "red" )
> ## term_frequency is loaded into your workspace
> library(wordcloud)
> # Print the first 10 entries in term_frequency
> term_frequency[1:10]
chardonnay     marvin       wine       gaye       just      glass       like 
       824        104         83         76         75         63         52 
    bottle        lol     little 
        47         43         35
> 
> # Vector of terms
> terms_vec<-names(term_frequency)
> 
> # Create a wordcloud for the values in word_freqs
> wordcloud(terms_vec, term_frequency, max.words = 50, colors = "red" )
> # Add to stopwords to remove some non-insightful terms.
> stops <- c(stopwords(kind = 'en'), 'chardonnay')
> # Review last 6 stopwords
> tail(stops)
[1] "same"       "so"         "than"       "too"        "very"      
[6] "chardonnay"
> 
> # Apply to a corpus
> cleaned_chardonnay_corp <- tm_map(chardonnay_corp, removeWords, stops)
> 
> # Review a "cleaned" tweet again
> content(cleaned_chardonnay_corp[[24]])
[1] " brought  marvin gaye  "
> content(chardonnay_corp[[24]])
[1] " brought  marvin gaye  chardonnay"
# Sort the chardonnay_words in descending order
sorted_chardonnay_words<- sort(chardonnay_words, decreasing = TRUE)
# Print the 6 most frequent chardonnay terms
head(sorted_chardonnay_words, 6)
# Get a terms vector
term_vec <- names(chardonnay_words)
# Create a wordcloud for the values in word_freqs
wordcloud(term_vec, chardonnay_words, 
          max.words = 50, colors = "red")

# Print the wordcloud with the specified colors,can specify a vector of colors to make certain words stand out or to fit an existing color scheme.
wordcloud(chardonnay_freqs$term, 
          chardonnay_freqs$num, 
          max.words = 100, 
          colors = c("grey80", "darkgoldenrod1", "tomato")
# We also can prebuilt palette with colors for improve cloud          
# Select 5 colors 
color_pal <- cividis(n=5)
color_pal
# Create a wordcloud with the selected palette
wordcloud(chardonnay_freqs$term, chardonnay_freqs$num, max.words = 100, colors = color_pal)

#Fined common words
# Create all_coffee
all_coffee <- paste(coffee_tweets$text, collapse = " ")
# Create all_chardonnay
all_chardonnay <- paste(chardonnay_tweets$text, collapse = " ")
# Create all_tweets
all_tweets <- c(all_coffee, all_chardonnay)
# Convert to a vector source
all_tweets <- VectorSource(all_tweets)
# Create all_corpus
all_corpus <- VCorpus(all_tweets)

## Plot common words cloud
> # Clean the corpus
> all_clean <- clean_corpus(all_corpus)
> # Create all_tdm
> all_tdm <- TermDocumentMatrix(all_clean)
> # Create all_m
> all_m <- as.matrix(all_tdm)
> # Print a commonality cloud
> commonality.cloud(all_m, max.words = 100, colors = "steelblue")

> # Plot of comparison to Visualize dissimilar words
> all_clean <- clean_corpus(all_corpus)
> # Create all_tdm
> all_tdm <- TermDocumentMatrix(all_clean)
> # Give the columns distinct names
> colnames(all_tdm) <- c("coffee", "chardonnay")
> # Create all_m
> all_m <- as.matrix(all_tdm)
> # Create comparison cloud
> comparison.cloud(all_m, colors = c("orange", "blue"), max.words = 50)
> 
> #Polarized tag cloud
> top25_df <- all_tdm_m %>%
    # Convert to data frame
    as_data_frame(rownames = "word") %>% 
    # Keep rows where word appears everywhere
    filter_all(all_vars(. > 0)) %>% 
    # Get difference in counts
    mutate(difference = chardonnay - coffee) %>% 
    # Keep rows with biggest difference
    top_n(25, wt = difference) %>% 
    # Arrange by descending difference
    arrange(desc(difference))
> 
> pyramid.plot(
    # Chardonnay counts
    top25_df$chardonnay, 
    # Coffee counts
    top25_df$coffee, 
    # Words
    labels = top25_df$word, 
    top.labels = c("Chardonnay", "Words", "Coffee"), 
    main = "Words in Common", 
    unit = NULL,
    gap = 8,
  )
[1] 5.1 4.1 4.1 2.1

# Word association
word_associate(chardonnay_tweets$text, match.string = "marvin", 
               stopwords = c(Top200Words, "chardonnay", "amp"), 
               network.plot = TRUE, cloud.colors = c("gray85", "darkred"))

# Add title
title(main = "Chardonnay Tweets Associated with Marvin")

> # Word association
> word_associate(coffee_tweets$text, match.string = "barista", 
                 stopwords = c(Top200Words, "coffee", "amp"), 
                 network.plot = TRUE, cloud.colors = c("gray85", "darkred"))
Warning message: length of colors should be 1 more than length of recode.words
  row group unit text                                                                                                                                
1 544   all  544 RT @Barista_kyo: #coffee #latte #soylatte #thinkcoffee # # # # @ think coffee http://t.co/Hmy9RPRWTZ                                
2 569   all  569 RT @ReversoSmith: What a beautiful mess! #portafilter #coffee #espresso #coffeemachine #barista #baristalife? http://t.co/ZODcTfP22Z
3 658   all  658 The moment you realize your Starbucks barista gave you a regular iced Coffee when u asked 4 decaf. Shitty. Late night not planned.  
4 931   all  931 Barista made my coffee wrong and still gave me both anyway #Starbucks #coffee #caffeine #upallnight http://t.co/iKCNwO8F6t          
5 951   all  951 RT @FrankIero: hahaha @jamiasan :*gives Barista our Starbucks order* Barista: coffee? @jamiasan : yes, isn't this is a coffee store?

Match Terms
===========

List 1:
baristakyo, barista, baristalife

> 
> # Add title
> title(main = "Barista Coffee Tweet Associations")

# Simple word clustering
# Plot a dendrogram
plot(hc)
> # Create dist_rain
> dist_rain <- dist(rain[,2])
> 
> # View the distance matrix
> dist_rain
      1     2     3
2  0.00            
3  4.63  4.63      
4 23.31 23.31 18.68
> 
> # Create hc
> hc <- hclust(dist_rain)
> 
> # Plot hc
> plot(hc, labels = rain$city)
> > # Print the dimensions of tweets_tdm. The professional settings a good dendrogram is based on a TDM with 25 to 70 terms. Having more than 70 terms may mean the visual will be cluttered and incomprehensible
> dim(tweets_tdm)
[1] 3207 1000
> # Create tdm1
>  tdm1 <-removeSparseTerms(tweets_tdm, sparse = 0.95)
> # Create tdm2
> tdm2 <- removeSparseTerms(tweets_tdm, sparse = 0.975)
> # Print tdm1
> tdm1
<<TermDocumentMatrix (terms: 25, documents: 1000)>>
Non-/sparse entries: 3778/21222
Sparsity           : 85%
Maximal term length: 10
Weighting          : term frequency (tf)
> # Print tdm2
> tdm2
<<TermDocumentMatrix (terms: 55, documents: 1000)>>
Non-/sparse entries: 4892/50108
Sparsity           : 91%
Maximal term length: 10
Weighting          : term frequency (tf)

# Create tweets_tdm2
tweets_tdm2 <- removeSparseTerms(tweets_tdm, sparse = 0.975)
# Create tdm_m
tdm_m <- as.matrix(tweets_tdm2)
# Create tweets_dist
tweets_dist <- dist(tdm_m)
# Create hc
hc <- hclust(tweets_dist)
# Plot the dendrogram
plot(hc)

# Try to improve the dendrogram
> hcd <- as.dendrogram(hc)
> # Print the labels in hcd
> labels(hcd)
 [1] "chardonnay" "you"        "to"         "is"         "amp"       
 [6] "this"       "gaye"       "marvin"     "my"         "with"      
[11] "it"         "for"        "on"         "just"       "me"        
[16] "that"       "glass"      "like"       "in"         "wine"      
[21] "a"          "of"         "the"        "and"        "i"
> # Change the branch color to red for "marvin" and "gaye"
> hcd_colored <- branches_attr_by_labels(hcd, c("marvin", "gaye"), color =  "red") 
> # Plot hcd
> plot(hcd_colored, main = "Better Dendrogram")
> # Add cluster rectangles where “marvin” and “gaye” branches are highlighted red.
> rect.dendrogram(hcd_colored, k=2, border = "grey50")
> 
# Correlation is a powerful tool at your disposal.
> associations <- findAssocs(tweets_tdm, "venti", 0.2) # function will return a list of all other terms that meet or exceed the minimum threshold.
> # View the venti associations
> associations
$venti
    breve   drizzle    entire     pumps     extra       cuz    forget      okay 
     0.58      0.58      0.58      0.58      0.47      0.41      0.41      0.41 
    hyper     mocha   vanilla       wtf    always    asleep       get starbucks 
     0.33      0.33      0.33      0.29      0.26      0.26      0.25      0.25 
    white 
     0.23
> # Create associations_df
> associations_df <- list_vect2df(associations, col2 = "word",  col3 = "score")
> # Plot the associations_df values
> ggplot(associations_df, aes(score, word)) + 
    geom_point(size = 3) + 
    theme_gdocs()
 # Minimum correlation values are often relatively low because of word diversity. Don't be surprised if 0.10 demonstrates a strong pairwise term association.
 #Bigrams are a powerful way to explore text and useful derive phrases.
 # We also can make tokenizer customer function using RWeka library and achive good result increasing number of circles and terms in DTM and dicreasing non sparse entries
> tokenizer <- function(x){
    NGramTokenizer(x, Weka_control(min=2, max=2))
  }
> # Create unigram_dtm
> unigram_dtm <- DocumentTermMatrix(text_corp)
> # Create bigram_dtm
> bigram_dtm <- DocumentTermMatrix(text_corp, control = list(tokenize=tokenizer))
> # Print unigram_dtm
> unigram_dtm
<<DocumentTermMatrix (documents: 1000, terms: 3004)>>
Non-/sparse entries: 8065/2995935
Sparsity           : 100%
Maximal term length: 27
Weighting          : term frequency (tf)
> # Print bigram_dtm
> bigram_dtm
<<DocumentTermMatrix (documents: 1000, terms: 5555)>>
Non-/sparse entries: 7802/5547198
Sparsity           : 100%
Maximal term length: 40
Weighting          : term frequency (tf)
> 
# cleaned and organized into a DTM called bigram_dtm.
> bigram_dtm_m <- as.matrix(bigram_dtm)
> # Create freq
> freq <- colSums( bigram_dtm_m )
> # Create bi_words
> bi_words <- names(freq)
> # Examine part of bi_words
> str_subset(bi_words, "^marvin")
 [1] "marvin big"          "marvin bigsean"      "marvin eduaubdedubu"
 [4] "marvin feat"         "marvin gay"          "marvin gaye"        
 [7] "marvin got"          "marvin high"         "marvin now"         
[10] "marvin really"       "marvin song"         "marvin will"
> # Plot a wordcloud
> wordcloud(bi_words, freq, max.words = 15)
> 
> # Create a TDM to Change frequency weights
> tdm <- TermDocumentMatrix(text_corp)
> # Convert it to a matrix
> tdm_m <- as.matrix(tdm)
> # Examine part of the matrix
> tdm_m[c("coffee", "espresso", "latte"), 161:166]
          Docs
Terms      161 162 163 164 165 166
  coffee     1   1   1   2   0   1
  espresso   1   0   0   0   0   0
  latte      0   0   0   0   0   1
  
> If we change adding control with TFidf we can see how to changed result
> tdm_tfidf <- TermDocumentMatrix(text_corp, control = list(weighting = weightTfIdf))

> tdm_tfidf_M <- as.matrix(tdm)
> # Examine part of the matrix
> tdm_tfidf_m[c("coffee", "espresso", "latte"), 161:166]
          Docs
Terms              161         162         163        164 165         166
  coffee   0.005611691 0.008729297 0.005611691 0.02618789   0 0.009820459
  espresso 0.545989728 0.000000000 0.000000000 0.00000000   0 0.000000000
  latte    0.000000000 0.000000000 0.000000000 0.00000000   0 1.047602723
> 
