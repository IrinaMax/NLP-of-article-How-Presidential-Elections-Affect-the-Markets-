
#Cleaning and preprocessing text
#Two ways to make a corpus, you can focus on cleaning, or preprocessing, the text. First, you'll clean a small piece of text, then you will move on to larger corpora.

> # Create the object: text
> text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."
> 
> # Make lowercase
> tolower(text)
[1] "<b>she</b> woke up at       6 a.m. it's so early!  she was only 10% awake and began drinking coffee in front of her computer."
> 

# Word stemming and stem completion on a sentence
> # Remove punctuation
> removePunctuation(text)
[1] "bSheb woke up at       6 AM Its so early  She was only 10 awake and began drinking coffee in front of her computer"
> 
> # Remove numbers
> removeNumbers(text)
[1] "<b>She</b> woke up at        A.M. It's so early!  She was only % awake and began drinking coffee in front of her computer."
> # Remove whitespace
> stripWhitespace(text)
[1] "<b>She</b> woke up at 6 A.M. It's so early! She was only 10% awake and began drinking coffee in front of her computer."
# Cleaning with qdap
> head(text)
[1] "<b>She</b> woke up at       6 A.M. It's so early!  She was only 10% awake and began drinking coffee in front of her computer."
> # Remove text within brackets
> bracketX(text)
[1] "She woke up at 6 A.M. It's so early! She was only 10% awake and began drinking coffee in front of her computer."
> 
> # Replace numbers with words
> replace_number(text)
[1] "<b>She</b> woke up at six A.M. It's so early! She was only ten% awake and began drinking coffee in front of her computer."
> # Replace abbreviations
> replace_abbreviation(text)
[1] "<b>She</b> woke up at 6 AM It's so early! She was only 10% awake and began drinking coffee in front of her computer."
> 
> # Replace contractions
> replace_contraction(text)
[1] "<b>She</b> woke up at 6 A.M. it is so early! She was only 10% awake and began drinking coffee in front of her computer."
> 
> # Replace symbols with words
> replace_symbol(text)
[1] "<b>She</b> woke up at 6 A.M. It's so early! She was only 10 percent awake and began drinking coffee in front of her computer."
> ## stop wordsÑŽ Often there are words that are frequent but provide little information
> text
[1] "<b>She</b> woke up at       6 A.M. It's so early!  She was only 10% awake and began drinking coffee in front of her computer."
> # List standard English stop words
> stopwords("en")
  [1] "i"          "me"         "my"         "myself"     "we"        
  [6] "our"        "ours"       "ourselves"  "you"        "your"      
 [11] "yours"      "yourself"   "yourselves" "he"         "him"       
 [16] "his"        "himself"    "she"        "her"        "hers"      
 [21] "herself"    "it"         "its"        "itself"     "they"      
 [26] "them"       "their"      "theirs"     "themselves" "what"      
 [31] "which"      "who"        "whom"       "this"       "that"      
 [36] "these"      "those"      "am"         "is"         "are"       
 [41] "was"        "were"       "be"         "been"       "being"     
 [46] "have"       "has"        "had"        "having"     "do"        
 [51] "does"       "did"        "doing"      "would"      "should"    
 [56] "could"      "ought"      "i'm"        "you're"     "he's"      
 [61] "she's"      "it's"       "we're"      "they're"    "i've"      
 [66] "you've"     "we've"      "they've"    "i'd"        "you'd"     
 [71] "he'd"       "she'd"      "we'd"       "they'd"     "i'll"      
 [76] "you'll"     "he'll"      "she'll"     "we'll"      "they'll"   
 [81] "isn't"      "aren't"     "wasn't"     "weren't"    "hasn't"    
 [86] "haven't"    "hadn't"     "doesn't"    "don't"      "didn't"    
 [91] "won't"      "wouldn't"   "shan't"     "shouldn't"  "can't"     
 [96] "cannot"     "couldn't"   "mustn't"    "let's"      "that's"    
[101] "who's"      "what's"     "here's"     "there's"    "when's"    
[106] "where's"    "why's"      "how's"      "a"          "an"        
[111] "the"        "and"        "but"        "if"         "or"        
[116] "because"    "as"         "until"      "while"      "of"        
[121] "at"         "by"         "for"        "with"       "about"     
[126] "against"    "between"    "into"       "through"    "during"    
[131] "before"     "after"      "above"      "below"      "to"        
[136] "from"       "up"         "down"       "in"         "out"       
[141] "on"         "off"        "over"       "under"      "again"     
[146] "further"    "then"       "once"       "here"       "there"     
[151] "when"       "where"      "why"        "how"        "all"       
[156] "any"        "both"       "each"       "few"        "more"      
[161] "most"       "other"      "some"       "such"       "no"        
[166] "nor"        "not"        "only"       "own"        "same"      
[171] "so"         "than"       "too"        "very"
> 
> # Print text without standard stop words
> removeWords(text, stopwords("en"))
[1] "<b>She</b> woke         6 A.M. It's  early!  She   10% awake  began drinking coffee  front   computer."
> 
> # Add "coffee" and "bean" to the list: new_stops
> new_stops <- c("coffee", "bean", stopwords("en"))
> 
> # Remove stop words from text
> removeWords(text, new_stops)
[1] "<b>She</b> woke         6 A.M. It's  early!  She   10% awake  began drinking   front   computer."
> 
> # Create complicate
> complicate <- c( "complicated", "complication", "complicatedly")
> 
> # Perform word stemming: stem_doc
> stem_doc <- stemDocument(complicate)
> stem_doc
[1] "complic" "complic" "complic"
> # Create the completion dictionary: comp_dict
> comp_dict <- "complicate"
> 
> # Perform stem completion: complete_text
> complete_text <- stemCompletion(stem_doc, comp_dict)
> 
> # Print complete_text
> complete_text
     complic      complic      complic 
"complicate" "complicate" "complicate"
> 
> text_data
[1] "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."
> rm_punc <- removePunctuation(text_data)
> rm_punc
[1] "In a complicated haste Tom rushed to fix a new complication too complicatedly"
> # Create character vector: n_char_vec
> n_char_vec <- unlist(strsplit(rm_punc, split = " "))
> n_char_vec
 [1] "In"            "a"             "complicated"   "haste"        
 [5] "Tom"           "rushed"        "to"            "fix"          
 [9] "a"             "new"           "complication"  "too"          
[13] "complicatedly"
> # Perform word stemming: stem_doc
> stem_doc <- stemDocument(n_char_vec)
> 
> # Print stem_doc
> 
> complete_doc <- stemCompletion(stem_doc, comp_dict)
> 
> # Print complete_doc
> complete_doc
          In            a      complic         hast          Tom         rush 
        "In"          "a" "complicate"      "haste"        "Tom"       "rush" 
          to          fix            a          new      complic          too 
        "to"        "fix"          "a"        "new" "complicate"        "too" 
     complic 
"complicate"
# Make a function including all cleaning data
clean_corpus <- function(corpus) {
  # Remove punctuation
  corpus <- tm_map(corpus, removePunctuation)
  # Transform to lower case
  corpus <- tm_map(corpus, content_transformer(tolower))
  # Add more stopwords
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee", "mug"))
  # Strip whitespace
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}
> # Apply this function to the text
> clean_corpus <- function(corpus){
    corpus <- tm_map(corpus, removePunctuation)
    corpus <- tm_map(corpus, content_transformer(tolower))
    corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee", "mug"))
    corpus <- tm_map(corpus, stripWhitespace)
    return(corpus)
  }
> 
> # Appling customized function to the tweet_corp: clean_corp
> clean_corp <- clean_corpus(tweet_corp)
> 
> # Print out a cleaned up tweet
> clean_corp[[227]][1]
$content
[1] "also dogs arent smart enough dip donut eat part thats dipped ladyandthetramp"
> 
> # Print out the same tweet in original form
> tweets$text[227]
[1] "Also, dogs aren't smart enough to dip the donut in the coffee and then eat the part that's been dipped. #ladyandthetramp"
> # Create the dtm from the corpus: coffee_dtm
> 
> coffee_dtm <- DocumentTermMatrix(clean_corp)
> # Print out coffee_dtm data
> coffee_dtm
<<DocumentTermMatrix (documents: 1000, terms: 3101)>>
Non-/sparse entries: 7724/3093276
Sparsity           : 100%
Maximal term length: 33
Weighting          : term frequency (tf)
> 
> 
> # Convert coffee_dtm to a matrix: coffee_m
> coffee_m <- as.matrix(coffee_dtm)
> 
> # Print the dimensions of coffee_m
> dim(coffee_m)
[1] 1000 3101
> 
> # Review a portion of the matrix to get some Starbucks
> print(coffee_m[475:478, 2593:2594])
     Terms
Docs  star starbucks
  475    0         0
  476    0         0
  477    0         0
  478    0         2
> 
> # Create a TDM from clean_corp: coffee_tdm
> 
> coffee_tdm <- TermDocumentMatrix(clean_corp)
> # Print coffee_tdm data
> coffee_tdm
<<TermDocumentMatrix (terms: 3101, documents: 1000)>>
Non-/sparse entries: 7724/3093276
Sparsity           : 100%
Maximal term length: 33
Weighting          : term frequency (tf)
> 
> 
> # Convert coffee_tdm to a matrix: coffee_m
> coffee_m <- as.matrix(coffee_tdm)
> 
> # Print the dimensions of the matrix
> dim(coffee_m)
[1] 3101 1000
> 
> # Review a portion of the matrix
> coffee_m[2593:2594, 475:478]
           Docs
Terms       475 476 477 478
  star        0   0   0   0
  starbucks   0   0   0   2
> 
> ## coffee_tdm is still loaded in your workspace
> 
> # Create a matrix: coffee_m
> coffee_m  <-as.matrix(coffee_tdm)
> 
> # Calculate the rowSums: term_frequency
> term_frequency <- rowSums(coffee_m)
> 
> # Sort term_frequency in descending order
> term_frequency<- sort(term_frequency, decreasing = TRUE)
> 
> # View the top 10 most common words
> head(term_frequency,10)
    like      cup      one     shop     just      get  morning     want 
     111      103       79       69       66       62       57       49 
drinking      can 
      47       45
> 
> # term_frequency[1:10]
> # Plot a barchart of the 10 most common words
> barplot(term_frequency[1:10], col = "tan", las = 2)
> 
> # Make a frequency barchart
> plot(frequency)
> # fast way to get frequent terms is with freq_terms() from qdap
> frequency <- freq_terms(
    tweets$text, 
    top = 10, 
    at.least = 3, 
    stopwords = "Top200Words"
  )
> 
> # Make a frequency barchart
> plot(frequency)

# Make a frequency barchart
plot(frequency)
# Make a frequency barchart
> # Create frequency
> frequency <- freq_terms(
    tweets$text, 
    top = 10, 
    at.least = 3, 
    stopwords = stopwords("english")
  )
> 
> # Make a frequency barchart
> plot(frequency)
